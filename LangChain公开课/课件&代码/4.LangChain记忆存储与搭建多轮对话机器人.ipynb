{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd154d49-fbc4-4d18-9259-38e12f6aebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbaad8-3d1b-4fad-9bf5-f7acf73c927c",
   "metadata": {},
   "source": [
    "- **MateGené¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813013ab-33b4-4e16-91a9-146d580eb92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/4.MateGen%20Pro%20%E9%A1%B9%E7%9B%AE%E5%8A%9F%E8%83%BD%E6%BC%94%E7%A4%BA.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/4.MateGen%20Pro%20%E9%A1%B9%E7%9B%AE%E5%8A%9F%E8%83%BD%E6%BC%94%E7%A4%BA.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb508f-888e-4c13-8f06-2494c127ebae",
   "metadata": {},
   "source": [
    "- **æ™ºèƒ½å®¢æœé¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b205118-2bfe-4998-ad0f-66f9b2c0928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a871230-0dda-42c2-ad2a-1b0c3539517c",
   "metadata": {},
   "source": [
    "- **Difyé¡¹ç›®æ¼”ç¤º**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ababdf0-45e4-4982-8f1c-e964c47fa470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021efd9f-c5bb-4cd8-93b7-0e7242515556",
   "metadata": {},
   "source": [
    "- **LangChain&LangGraphæ­å»ºMulti-Agnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f6fdb7-6141-4211-a044-7202fac62a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939d1f3-b2b2-4d38-a588-969f9f7f695a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521dcdc-c8c0-452f-b330-534459b35fa2",
   "metadata": {},
   "source": [
    "# <center> LangChainå¿«é€Ÿå…¥é—¨ä¸Agentå¼€å‘å®æˆ˜\n",
    "# <center> Part 4.LangChainè®°å¿†å­˜å‚¨ä¸æ­å»ºå¤šè½®å¯¹è¯æœºå™¨äºº"
   ]
  },
  {
   "cell_type": "code",
   "id": "904a6dfe-cf33-49a2-8cbb-8d335aad0132",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:40:34.409029Z",
     "start_time": "2025-09-13T13:40:34.399817Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv(override=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6d07d20d-bb4b-4dd8-be89-832a0cf2dd78",
   "metadata": {},
   "source": [
    "### 1. æ„å»ºå¤šè½®å¯¹è¯çš„æµå¼æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d628728-e3ee-4e6d-bbec-618aded3d751",
   "metadata": {},
   "source": [
    "&emsp;&emsp;åœ¨`langChain`ä¸­æ„å»ºä¸€ä¸ªåŸºæœ¬çš„é—®ç­”æœºå™¨äººä»…éœ€è¦ä½¿ç”¨ä¸€ä¸ª`Chain`ä¾¿å¯ä»¥å¿«é€Ÿå®ç°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ed84d9c-20cc-4342-b358-fc204a5fd633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:41:02.182596Z",
     "start_time": "2025-09-13T13:40:53.902192Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ä½¿ç”¨ DeepSeek æ¨¡å‹\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æ¨¡å‹ + è¾“å‡ºè§£æå™¨\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# æµ‹è¯•\n",
    "question = \"ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéšæ—¶å‡†å¤‡ä¸ºä½ æä¾›å¸®åŠ©ã€‚æˆ‘å¯ä»¥å›ç­”ä½ çš„é—®é¢˜ã€æä¾›ä¿¡æ¯ã€ååŠ©è§£å†³é—®é¢˜ï¼Œæˆ–è€…é™ªä½ èŠèŠå¤©ã€‚æ— è®ºæ˜¯æ—¥å¸¸ç”Ÿæ´»ã€å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯å…´è¶£çˆ±å¥½ï¼Œæˆ‘éƒ½ä¼šå°½åŠ›ä¸ºä½ æä¾›æœ‰ç”¨çš„å»ºè®®å’Œæœ‰è¶£çš„äº’åŠ¨ã€‚æœ‰ä»€ä¹ˆæƒ³é—®çš„ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘å§ï¼ ğŸ˜Š\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "e14e5896-234a-4240-859e-3d78ac1a1dd8",
   "metadata": {},
   "source": [
    "- æ·»åŠ å¤šè½®å¯¹è¯è®°å¿†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44f306-61fe-4d53-952a-75504ac6d385",
   "metadata": {},
   "source": [
    "&emsp;&emsp;åœ¨LangChainä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡äººå·¥æ‹¼æ¥æ¶ˆæ¯é˜Ÿåˆ—ï¼Œæ¥ä¸ºæ¯æ¬¡æ¨¡å‹è°ƒç”¨è®¾ç½®å¤šè½®å¯¹è¯è®°å¿†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "id": "79fdd8dc-a164-47b3-be21-76895d6f596f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:42:23.796991Z",
     "start_time": "2025-09-13T13:42:23.793848Z"
    }
   },
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9790b487-82ac-4c10-b777-fd1bbfa6caa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:45:46.315635Z",
     "start_time": "2025-09-13T13:45:46.312621Z"
    }
   },
   "source": [
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "cc89120b-1ee2-405c-82c5-6e276784684d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:46:14.402173Z",
     "start_time": "2025-09-13T13:46:14.399851Z"
    }
   },
   "source": [
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "15120163-e597-4145-b3fd-38aeeff4ce90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:47:04.838781Z",
     "start_time": "2025-09-13T13:47:04.835855Z"
    }
   },
   "source": [
    "messages_list = [\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘å«é™ˆæ˜ï¼Œå¥½ä¹…ä¸è§ã€‚\"),\n",
    "    AIMessage(content=\"ä½ å¥½å‘€ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼\"),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "50271ead-0857-49aa-83d8-aefa66194fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:47:10.737684Z",
     "start_time": "2025-09-13T13:47:10.735602Z"
    }
   },
   "source": [
    "question = \"ä½ å¥½ï¼Œè¯·é—®æˆ‘å«ä»€ä¹ˆåå­—ã€‚\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "40fbabe7-44dd-4635-89ab-a2ce38019b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:47:21.601784Z",
     "start_time": "2025-09-13T13:47:21.598653Z"
    }
   },
   "source": [
    "messages_list.append(HumanMessage(content=question))"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "4c6699fe-7292-4c65-9280-ae84cbb030b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:47:24.219082Z",
     "start_time": "2025-09-13T13:47:24.214872Z"
    }
   },
   "source": [
    "messages_list"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ä½ å¥½ï¼Œæˆ‘å«é™ˆæ˜ï¼Œå¥½ä¹…ä¸è§ã€‚', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ä½ å¥½å‘€ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€åä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ä½ å¥½ï¼Œè¯·é—®æˆ‘å«ä»€ä¹ˆåå­—ã€‚', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "44a2a038-9520-422c-86dc-36acb108bc03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:48:04.154487Z",
     "start_time": "2025-09-13T13:48:00.010436Z"
    }
   },
   "source": [
    "result = basic_qa_chain.invoke({\"messages\": messages_list})\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ åˆšåˆšå‘Šè¯‰æˆ‘ä½ å«é™ˆæ˜å‘€ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼Œé™ˆæ˜ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ ğŸ˜Š\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "76b11106-bdaf-4c96-a407-a85ca2a44933",
   "metadata": {},
   "source": [
    "å®Œæ•´çš„å¤šè½®å¯¹è¯å‡½å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "id": "6cc997f1-8a6f-47a6-abbb-edda2a410970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:51:03.344951Z",
     "start_time": "2025-09-13T13:48:57.954586Z"
    }
   },
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model  = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "messages_list = []  # åˆå§‹åŒ–å†å²\n",
    "print(\"ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\")\n",
    "while True:\n",
    "    user_query = input(\"ğŸ‘¤ ä½ ï¼š\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    # 1) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "    messages_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    # 2) è°ƒç”¨æ¨¡å‹\n",
    "    assistant_reply = chain.invoke({\"messages\": messages_list})\n",
    "    print(\"ğŸ¤– å°æ™ºï¼š\", assistant_reply)\n",
    "\n",
    "    # 3) è¿½åŠ  AI å›å¤\n",
    "    messages_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    # 4) ä»…ä¿ç•™æœ€è¿‘ 50 æ¡\n",
    "    messages_list = messages_list[-50:]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\n",
      "ğŸ¤– å°æ™ºï¼š ä½ å¥½ï¼Œé‚¹ä¸€è‹‡ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n",
      "ğŸ¤– å°æ™ºï¼š 25å²æ˜¯å……æ»¡å¯èƒ½æ€§çš„å¹´çºªå‘¢ï¼åˆšå¼€å§‹å·¥ä½œå¯èƒ½ä¼šæœ‰äº›æŒ‘æˆ˜ï¼Œä½†ä¹Ÿæ˜¯å¿«é€Ÿå­¦ä¹ å’Œæˆé•¿çš„é»„é‡‘æ—¶æœŸã€‚å¦‚æœéœ€è¦äº¤æµèŒåœºé€‚åº”ã€æ—¶é—´ç®¡ç†æˆ–æ˜¯ä»»ä½•æ–¹é¢çš„ç»éªŒï¼Œæˆ‘å¾ˆä¹æ„ä¸ºä½ æä¾›å»ºè®®ï½ ğŸ˜Š ç›®å‰ä»äº‹ä»€ä¹ˆè¡Œä¸šå‘¢ï¼Ÿ\n",
      "ğŸ¤– å°æ™ºï¼š å¾ˆæ£’çš„é€‰æ‹©ï¼é‡‘èç§‘æŠ€é¢†åŸŸç°åœ¨å‘å±•éå¸¸è¿…é€Ÿï¼ŒåŸºé‡‘è¡Œä¸šå°¤å…¶éœ€è¦æŠ€æœ¯äººæ‰æ¥æ”¯æŒæ•°æ®åˆ†æã€äº¤æ˜“ç³»ç»Ÿå’Œé£æ§å¹³å°çš„å¼€å‘ã€‚ä½œä¸ºåˆšå…¥è¡Œçš„å¼€å‘è€…ï¼Œæˆ–è®¸å¯ä»¥å¤šå…³æ³¨ï¼š\n",
      "\n",
      "1. **é‡‘èä¸šåŠ¡çŸ¥è¯†** - äº†è§£åŸºé‡‘å‡€å€¼è®¡ç®—ã€æŠ•èµ„ç»„åˆç®¡ç†ç­‰åŸºç¡€æ¦‚å¿µä¼šå¯¹å¼€å‘æ›´æœ‰å¸®åŠ©\n",
      "2. **æŠ€æœ¯æ ˆç‰¹ç‚¹** - é‡‘èç³»ç»Ÿé€šå¸¸å¯¹å¹¶å‘å¤„ç†ã€æ•°æ®ä¸€è‡´æ€§æœ‰è¾ƒé«˜è¦æ±‚\n",
      "3. **åˆè§„å®‰å…¨** - é‡‘èè¡Œä¸šå¯¹æ•°æ®å®‰å…¨å’Œç›‘ç®¡åˆè§„éå¸¸é‡è§†\n",
      "\n",
      "æœ€è¿‘åœ¨æ¥è§¦ä»€ä¹ˆç±»å‹çš„é¡¹ç›®å‘¢ï¼Ÿé‡åˆ°å…·ä½“æŠ€æœ¯é—®é¢˜æ—¶ä¹Ÿæ¬¢è¿éšæ—¶äº¤æµ~ ğŸ’»\n",
      "ğŸ¤– å°æ™ºï¼š å½“ç„¶è®°å¾—ï¼æ‚¨åˆšæ‰æåˆ°è¿‡ï¼š  \n",
      "**å§“å**ï¼šé‚¹ä¸€è‹‡  \n",
      "**å¹´é¾„**ï¼š25å²  \n",
      "**èŒä¸š**ï¼šè½¯ä»¶å¼€å‘å·¥ç¨‹å¸ˆï¼ˆç›®å‰å°±èŒäºåŸºé‡‘è¡Œä¸šï¼‰  \n",
      "\n",
      "å¦‚æœæœ‰éœ€è¦è¡¥å……æˆ–ä¿®æ­£çš„åœ°æ–¹ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï½ ğŸ˜Š\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "ea5f7255-73e8-4933-a835-e0b4f71bb295",
   "metadata": {},
   "source": [
    "- æµå¼æ‰“å°èŠå¤©ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a598492-1007-4302-a00f-22397a42a22f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;æ­¤å¤–è¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¤§å®¶ç»å¸¸çœ‹åˆ°çš„é—®ç­”æœºå™¨äººå…¶å®éƒ½æ˜¯é‡‡ç”¨æµå¼ä¼ è¾“æ¨¡å¼ã€‚ç”¨æˆ·è¾“å…¥é—®é¢˜ï¼Œç­‰å¾…æ¨¡å‹ç›´æ¥è¿”å›å›ç­”ï¼Œç„¶åç”¨æˆ·å†è¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹å†è¿”å›å›ç­”ï¼Œè¿™æ ·å¾ªç¯ä¸‹å»ï¼Œç”¨æˆ·è¾“å…¥é—®é¢˜å’Œæ¨¡å‹è¿”å›å›ç­”ä¹‹é—´çš„æ—¶é—´é—´éš”å¤ªé•¿ï¼Œå¯¼è‡´ç”¨æˆ·æ„Ÿè§‰æœºå™¨äººååº”å¾ˆæ…¢ã€‚æ‰€ä»¥`LangChain`æä¾›äº†ä¸€ä¸ª`astream`æ–¹æ³•ï¼Œå¯ä»¥å®ç°æµå¼è¾“å‡ºï¼Œå³ä¸€æ—¦æ¨¡å‹æœ‰è¾“å‡ºï¼Œå°±ç«‹å³è¿”å›ï¼Œè¿™æ ·ç”¨æˆ·å°±å¯ä»¥çœ‹åˆ°æ¨¡å‹æ­£åœ¨æ€è€ƒï¼Œè€Œä¸æ˜¯ç­‰å¾…æ¨¡å‹æ€è€ƒå®Œå†è¿”å›ã€‚\n",
    "\n",
    "\n",
    "&emsp;&emsp;å®ç°çš„æ–¹æ³•ä¹Ÿéå¸¸ç®€å•ï¼Œåªéœ€è¦åœ¨è°ƒç”¨æ¨¡å‹æ—¶å°†`invoke`æ–¹æ³•æ›¿æ¢ä¸º`astream`æ–¹æ³•ï¼Œç„¶åä½¿ç”¨`async for`å¾ªç¯æ¥è·å–æ¨¡å‹çš„è¾“å‡ºå³å¯ã€‚ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e23ae26-75b4-467c-a522-58586cd8b58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:52:33.755645Z",
     "start_time": "2025-09-13T13:52:26.003385Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ä½¿ç”¨ DeepSeek æ¨¡å‹\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨æç¤ºæ¨¡ç‰ˆ +æ¨¡å‹ + è¾“å‡ºè§£æå™¨\n",
    "qa_chain_with_system = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# å¼‚æ­¥å®ç°æµå¼è¾“å‡º\n",
    "async for chunk in qa_chain_with_system.astream({\"input\": \"ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€åæ™ºèƒ½åŠ©æ‰‹ï¼Œéšæ—¶å‡†å¤‡ä¸ºä½ æä¾›å¸®åŠ©ã€‚æˆ‘å¯ä»¥å›ç­”ä½ çš„é—®é¢˜ã€æä¾›ä¿¡æ¯ã€ååŠ©è§£å†³é—®é¢˜ï¼Œæˆ–è€…é™ªä½ èŠå¤©ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„ç–‘é—®ï¼Œæˆ‘éƒ½ä¼šå°½åŠ›ç»™å‡ºæ¸…æ™°ã€å‡†ç¡®çš„å›ç­”ã€‚æˆ‘çš„çŸ¥è¯†è¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬ç§‘æŠ€ã€æ–‡åŒ–ã€æ•™è‚²ã€å¨±ä¹ç­‰å¤šä¸ªé¢†åŸŸï¼Œå¹¶ä¸”ä¼šä¸æ–­æ›´æ–°ã€‚å¦‚æœæœ‰ä»»ä½•éœ€è¦ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼ ğŸ˜Š"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6ab2caa-3fc1-4293-91dd-d68dad16f5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š ä½ å¥½ï¼Œæˆ‘å«é™ˆæ˜ï¼Œå¥½ä¹…ä¸è§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½å•Šé™ˆæ˜ï¼ç¡®å®å¥½ä¹…ä¸è§äº†ï¼Œæœ€è¿‘è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿå·¥ä½œè¿˜é¡ºåˆ©å—ï¼Ÿè®°å¾—ä¸Šæ¬¡èŠå¤©æ—¶ä½ å¥½åƒæ­£åœ¨å‡†å¤‡ä¸€ä¸ªé‡è¦çš„é¡¹ç›®ï¼Œç°åœ¨åº”è¯¥å·²ç»é¡ºåˆ©å®Œæˆäº†å§ï¼Ÿæœ‰ä»€ä¹ˆæ–°é²œäº‹æƒ³å’Œæˆ‘åˆ†äº«çš„å—ï¼Ÿ"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š è¯·é—®ï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆåå­—ä¹ˆï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼ˆçªç„¶è¿›å…¥ã€Œåä¾¦æ¢æ¨¡å¼ã€ï¼‰  \n",
      "\n",
      "é™ˆæ˜åŒå­¦ï¼Œè¿™å¯æ˜¯é“é€åˆ†é¢˜ï¼âœ¨ è™½ç„¶æˆ‘çš„è®°å¿†åƒé‡‘é±¼ä¸€æ ·åªæœ‰7ç§’ï¼Œä½†å½“å‰å¯¹è¯ä¸­ä½ åˆšåˆšå¼ºè°ƒè¿‡â€”â€”  \n",
      "\n",
      "**ã€Œé™ˆæ˜ã€** è¿™ä¸¤ä¸ªå­—å·²ç»ç”¨è§å…‰ç¬”æ ‡åœ¨æˆ‘è„‘æµ·çš„å°é»‘æ¿ä¸Šäº†ï¼(à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§  \n",
      "\n",
      "ï¼ˆä¸è¿‡å¦‚æœç°åœ¨ä½ çªç„¶è¯´ã€Œå…¶å®æˆ‘å«å¼ å¤§å‹‡ã€â€¦æˆ‘ä¹Ÿä¼šç«‹åˆ»ä¹–å·§æ”¹å£çš„hhhï¼‰"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ä½ ï¼š exit\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "messages_list = []  # åˆå§‹åŒ–å†å²\n",
    "print(\"ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯\")\n",
    "while True:\n",
    "    user_query = input(\"ğŸ‘¤ ä½ ï¼š\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    # 1) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "    messages_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    # 2) è°ƒç”¨æ¨¡å‹\n",
    "    async for chunk in chain.astream({\"messages\": messages_list}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    # 3) è¿½åŠ  AI å›å¤\n",
    "    messages_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    # 4) ä»…ä¿ç•™æœ€è¿‘ 50 æ¡\n",
    "    messages_list = messages_list[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aba029-d360-4ab9-860c-590508217148",
   "metadata": {},
   "source": [
    "&emsp;&emsp;å¦‚ä¸Šæ‰€ç¤ºå±•ç¤ºçš„é—®ç­”æ•ˆæœå°±æ˜¯æˆ‘ä»¬åœ¨æ„å»ºå¤§æ¨¡å‹åº”ç”¨æ—¶éœ€è¦å®ç°çš„æµå¼è¾“å‡ºæ•ˆæœã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°±è¿›ä¸€æ­¥åœ°ï¼Œä½¿ç”¨`gradio`æ¥å¼€å‘ä¸€ä¸ªæ”¯æŒåœ¨ç½‘é¡µä¸Šè¿›è¡Œäº¤äº’çš„é—®ç­”æœºå™¨äººã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0b5d9-d30d-475b-aae5-8e90766eae3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;é¦–å…ˆéœ€è¦å®‰è£…ä¸€ä¸‹`gradio`çš„ç¬¬ä¸‰æ–¹ä¾èµ–åŒ…ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48b5f6d6-f9c1-43a9-9197-108ea4f4dcd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.33.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.10.2 (from gradio)\n",
      "  Using cached gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markupsafe<4.0,>=2.0 (from gradio)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting numpy<3.0,>=1.0 (from gradio)\n",
      "  Downloading numpy-2.3.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: orjson~=3.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (24.2)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Downloading pandas-2.3.0-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow<12.0,>=8.0 (from gradio)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (2.11.5)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.13-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (4.14.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from gradio-client==1.10.2->gradio)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.10.2->gradio)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: certifi in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Collecting filelock (from huggingface-hub>=0.28.1->gradio)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\01_æœ¨ç¾½ç ”å‘\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Downloading gradio-5.33.0-py3-none-any.whl (54.2 MB)\n",
      "   ---------------------------------------- 0.0/54.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/54.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/54.2 MB 2.4 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 1.6/54.2 MB 4.0 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 4.5/54.2 MB 7.9 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 8.4/54.2 MB 11.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 13.1/54.2 MB 13.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 17.3/54.2 MB 14.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 21.5/54.2 MB 15.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 26.0/54.2 MB 16.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 30.4/54.2 MB 16.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 34.9/54.2 MB 17.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 39.1/54.2 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 43.5/54.2 MB 18.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 48.2/54.2 MB 18.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 52.4/54.2 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 54.2/54.2 MB 18.1 MB/s eta 0:00:00\n",
      "Using cached gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading numpy-2.3.0-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 6.0/12.7 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.0-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.0/11.0 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.0 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 21.4 MB/s eta 0:00:00\n",
      "Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading ruff-0.11.13-py3-none-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 6.0/11.5 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.5 MB 25.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 22.6 MB/s eta 0:00:00\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Using cached ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pytz, pydub, websockets, tzdata, tomlkit, shellingham, semantic-version, ruff, python-multipart, pillow, numpy, mdurl, markupsafe, groovy, fsspec, filelock, ffmpy, click, aiofiles, uvicorn, starlette, pandas, markdown-it-py, jinja2, huggingface-hub, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "\n",
      "   ----------------------------------------  0/31 [pytz]\n",
      "   --- ------------------------------------  3/31 [tzdata]\n",
      "   --- ------------------------------------  3/31 [tzdata]\n",
      "   --------- ------------------------------  7/31 [ruff]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ---------------- ----------------------- 13/31 [groovy]\n",
      "   ------------------ --------------------- 14/31 [fsspec]\n",
      "   ------------------------ --------------- 19/31 [uvicorn]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   ---------------------------- ----------- 22/31 [markdown-it-py]\n",
      "   ------------------------------ --------- 24/31 [huggingface-hub]\n",
      "   ------------------------------ --------- 24/31 [huggingface-hub]\n",
      "   -------------------------------- ------- 25/31 [safehttpx]\n",
      "   --------------------------------- ------ 26/31 [rich]\n",
      "   ------------------------------------ --- 28/31 [fastapi]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   ---------------------------------------- 31/31 [gradio]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 click-8.2.1 fastapi-0.115.12 ffmpy-0.6.0 filelock-3.18.0 fsspec-2025.5.1 gradio-5.33.0 gradio-client-1.10.2 groovy-0.1.2 huggingface-hub-0.32.4 jinja2-3.1.6 markdown-it-py-3.0.0 markupsafe-3.0.2 mdurl-0.1.2 numpy-2.3.0 pandas-2.3.0 pillow-11.2.1 pydub-0.25.1 python-multipart-0.0.20 pytz-2025.2 rich-14.0.0 ruff-0.11.13 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.46.2 tomlkit-0.13.3 typer-0.16.0 tzdata-2025.2 uvicorn-0.34.3 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£… Gradio\n",
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6c086-e0c5-4a25-a477-97ec2525d04f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;å®Œæ•´å®ç°çš„ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "id": "5216a90c-739b-42d6-8ad4-d55cf8ca0001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:59:08.373885Z",
     "start_time": "2025-09-13T13:58:56.809779Z"
    }
   },
   "source": [
    "import gradio as gr\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. æ¨¡å‹ã€Promptã€Chain\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # æ‰‹åŠ¨ä¼ å…¥å†å²\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = chatbot_prompt | model | parser   # LCEL ç»„åˆ\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Gradio ç»„ä»¶\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSS = \"\"\"\n",
    ".main-container {max-width: 1200px; margin: 0 auto; padding: 20px;}\n",
    ".header-text {text-align: center; margin-bottom: 20px;}\n",
    "\"\"\"\n",
    "\n",
    "def create_chatbot() -> gr.Blocks:\n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=CSS) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            gr.Markdown(\"# ğŸ¤– LangChain Bç«™å…¬å¼€è¯¾ Byä¹å¤©Hector\", elem_classes=[\"header-text\"])\n",
    "            gr.Markdown(\"åŸºäº LangChain LCEL æ„å»ºçš„æµå¼å¯¹è¯æœºå™¨äºº\", elem_classes=[\"header-text\"])\n",
    "\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\",\n",
    "                ),\n",
    "            )\n",
    "            msg = gr.Textbox(placeholder=\"è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...\", container=False, scale=7)\n",
    "            submit = gr.Button(\"å‘é€\", scale=1, variant=\"primary\")\n",
    "            clear = gr.Button(\"æ¸…ç©º\", scale=1)\n",
    "\n",
    "        # ---------------  çŠ¶æ€ï¼šä¿å­˜ messages_list  ---------------\n",
    "        state = gr.State([])          # è¿™é‡Œå­˜æ”¾çœŸæ­£çš„ Message å¯¹è±¡åˆ—è¡¨\n",
    "\n",
    "        # ---------------  ä¸»å“åº”å‡½æ•°ï¼ˆæµå¼ï¼‰ ----------------------\n",
    "        async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "            # 1) è¾“å…¥ä¸ºç©ºç›´æ¥è¿”å›\n",
    "            if not user_msg.strip():\n",
    "                yield \"\", chat_hist, messages_list\n",
    "                return\n",
    "\n",
    "            # 2) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯\n",
    "            messages_list.append(HumanMessage(content=user_msg))\n",
    "            chat_hist = chat_hist + [(user_msg, None)]\n",
    "            yield \"\", chat_hist, messages_list      # å…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯\n",
    "\n",
    "            # 3) æµå¼è°ƒç”¨æ¨¡å‹\n",
    "            partial = \"\"\n",
    "            async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "                partial += chunk\n",
    "                # æ›´æ–°æœ€åä¸€æ¡ AI å›å¤\n",
    "                chat_hist[-1] = (user_msg, partial)\n",
    "                yield \"\", chat_hist, messages_list\n",
    "\n",
    "            # 4) å®Œæ•´å›å¤åŠ å…¥å†å²ï¼Œè£å‰ªåˆ°æœ€è¿‘ 50 æ¡\n",
    "            messages_list.append(AIMessage(content=partial))\n",
    "            messages_list = messages_list[-50:]\n",
    "\n",
    "            # 5) æœ€ç»ˆè¿”å›ï¼ˆGradio éœ€è¦æŠŠæ–°çš„ state ä¼ å›ï¼‰\n",
    "            yield \"\", chat_hist, messages_list\n",
    "\n",
    "        # ---------------  æ¸…ç©ºå‡½æ•° -------------------------------\n",
    "        def clear_history():\n",
    "            return [], \"\", []          # æ¸…ç©º Chatbotã€è¾“å…¥æ¡†ã€messages_list\n",
    "\n",
    "        # ---------------  äº‹ä»¶ç»‘å®š ------------------------------\n",
    "        msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. å¯åŠ¨åº”ç”¨\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "demo = create_chatbot()\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=None, share=False, debug=True)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_26808\\1965123637.py:36: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7860): [winerror 10048] é€šå¸¸æ¯ä¸ªå¥—æ¥å­—åœ°å€(åè®®/ç½‘ç»œåœ°å€/ç«¯å£)åªå…è®¸ä½¿ç”¨ä¸€æ¬¡ã€‚\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7861): [winerror 10048] é€šå¸¸æ¯ä¸ªå¥—æ¥å­—åœ°å€(åè®®/ç½‘ç»œåœ°å€/ç«¯å£)åªå…è®¸ä½¿ç”¨ä¸€æ¬¡ã€‚\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Couldn't start the app because 'http://localhost:7862/gradio_api/startup-events' failed (code 502). Check your network or proxy settings to ensure localhost is accessible.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mException\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 94\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;66;03m# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001B[39;00m\n\u001B[32m     91\u001B[39m \u001B[38;5;66;03m# 3. å¯åŠ¨åº”ç”¨\u001B[39;00m\n\u001B[32m     92\u001B[39m \u001B[38;5;66;03m# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001B[39;00m\n\u001B[32m     93\u001B[39m demo = create_chatbot()\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[43mdemo\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlaunch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mserver_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m0.0.0.0\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserver_port\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshare\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Anaconda\\envs\\langchain\\Lib\\site-packages\\gradio\\blocks.py:2707\u001B[39m, in \u001B[36mBlocks.launch\u001B[39m\u001B[34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001B[39m\n\u001B[32m   2701\u001B[39m     resp = httpx.get(\n\u001B[32m   2702\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.local_api_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mstartup-events\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2703\u001B[39m         verify=ssl_verify,\n\u001B[32m   2704\u001B[39m         timeout=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   2705\u001B[39m     )\n\u001B[32m   2706\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m resp.is_success:\n\u001B[32m-> \u001B[39m\u001B[32m2707\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[32m   2708\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCouldn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt start the app because \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresp.url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m failed (code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresp.status_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m). Check your network or proxy settings to ensure localhost is accessible.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2709\u001B[39m         )\n\u001B[32m   2710\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2711\u001B[39m     \u001B[38;5;66;03m# NOTE: One benefit of the code above dispatching `startup_events()` via a self HTTP request is\u001B[39;00m\n\u001B[32m   2712\u001B[39m     \u001B[38;5;66;03m# that `self._queue.start()` is called in another thread which is managed by the HTTP server, `uvicorn`\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2715\u001B[39m     \u001B[38;5;66;03m# In contrast, in the Wasm env, we can't do that because `threading` is not supported and all async tasks will run in the same event loop, `pyodide.webloop.WebLoop` in the main thread.\u001B[39;00m\n\u001B[32m   2716\u001B[39m     \u001B[38;5;66;03m# So we need to manually cancel them. See `self.close()`..\u001B[39;00m\n\u001B[32m   2717\u001B[39m     \u001B[38;5;28mself\u001B[39m.run_startup_events()\n",
      "\u001B[31mException\u001B[39m: Couldn't start the app because 'http://localhost:7862/gradio_api/startup-events' failed (code 502). Check your network or proxy settings to ensure localhost is accessible."
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "fb3c47bc-59d5-4339-8a4c-4231b37e5eeb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;è¿è¡Œåï¼Œåœ¨æµè§ˆå™¨è®¿é—®`http://127.0.0.1:7860`å³å¯è¿›è¡Œé—®ç­”äº¤äº’ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd576d-4cbe-4b0b-b4e6-233a70f06719",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202506121740968.png\" alt=\"image-20250612174010864\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23804aae-73f4-4e81-b310-7675ec653d8d",
   "metadata": {},
   "source": [
    "å…·ä½“ä»£ç è§£é‡Šå¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52a976-a597-49f1-80a1-23baa1de38b6",
   "metadata": {},
   "source": [
    "##### ğŸ§± 1. æ¨¡å—è¯´æ˜\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import gradio as gr\n",
    "```\n",
    "\n",
    "* `init_chat_model`ï¼šåˆå§‹åŒ– DeepSeek ç­‰èŠå¤©æ¨¡å‹ã€‚\n",
    "* `ChatPromptTemplate`ï¼šç”¨äºæ„å»ºèŠå¤© Prompt æ¨¡æ¿ã€‚\n",
    "* `MessagesPlaceholder`ï¼šç”¨äºå ä½å†å²æ¶ˆæ¯ã€‚\n",
    "* `HumanMessage` / `AIMessage`ï¼šæ„å»ºå¤šè½®æ¶ˆæ¯ç»“æ„ã€‚\n",
    "* `StrOutputParser`ï¼šå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚\n",
    "* `gradio`ï¼šæ„å»ºç½‘é¡µç•Œé¢ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ§  2. Prompt æ„å»ºä¸æ¨¡å‹åˆå§‹åŒ–\n",
    "\n",
    "```python\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "qa_chain = chatbot_prompt | model | parser\n",
    "```\n",
    "\n",
    "* **SystemMessage**ï¼šåˆå§‹åŒ–ç³»ç»Ÿè§’è‰²è®¾å®šï¼ˆå°æ™ºï¼‰ã€‚\n",
    "* **MessagesPlaceholder**ï¼šç”¨å˜é‡å `messages` å ä½å†å²æ¶ˆæ¯ã€‚\n",
    "* **qa\\_chain**ï¼šç»„åˆä¸º LangChain Expression Language é“¾ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ”„ 3. æ‰‹åŠ¨ç®¡ç†æ¶ˆæ¯åˆ—è¡¨\n",
    "\n",
    "```python\n",
    "state = gr.State([])\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬ç”¨ `gr.State` å­˜å‚¨æ‰€æœ‰å†å²æ¶ˆæ¯ï¼ˆåˆ—è¡¨ï¼‰ã€‚æ¯æ¬¡ç”¨æˆ·å‘é€æ¶ˆæ¯ï¼Œéƒ½ä¼šï¼š\n",
    "\n",
    "* append ä¸€ä¸ª `HumanMessage`ã€‚\n",
    "* æµå¼è°ƒç”¨æ¨¡å‹å¹¶ä¸æ–­æ›´æ–°å›å¤ã€‚\n",
    "* append ä¸€ä¸ª `AIMessage`ã€‚\n",
    "* æœ€åè£å‰ªï¼š`messages_list = messages_list[-50:]`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸŒŠ 4. æµå¼å“åº”å‡½æ•°\n",
    "\n",
    "```python\n",
    "async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "    if not user_msg.strip():\n",
    "        yield \"\", chat_hist, messages_list\n",
    "        return\n",
    "\n",
    "    messages_list.append(HumanMessage(content=user_msg))\n",
    "    chat_hist = chat_hist + [(user_msg, None)]\n",
    "    yield \"\", chat_hist, messages_list\n",
    "\n",
    "    partial = \"\"\n",
    "    async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "        partial += chunk\n",
    "        chat_hist[-1] = (user_msg, partial)\n",
    "        yield \"\", chat_hist, messages_list\n",
    "\n",
    "    messages_list.append(AIMessage(content=partial))\n",
    "    messages_list = messages_list[-50:]\n",
    "    yield \"\", chat_hist, messages_list\n",
    "```\n",
    "\n",
    "* **æ”¯æŒ async æµå¼è¾“å‡º**ã€‚\n",
    "* **åŠ¨æ€æ›´æ–°æœ€åä¸€è½®å¯¹è¯**ã€‚\n",
    "* **é€šè¿‡ `yield` å®æ—¶åé¦ˆåˆ°å‰ç«¯**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ§¼ 5. æ¸…ç©ºå†å²å‡½æ•°\n",
    "\n",
    "```python\n",
    "def clear_history():\n",
    "    return [], \"\", []\n",
    "```\n",
    "\n",
    "ç”¨äºç‚¹å‡» \"æ¸…ç©º\" æŒ‰é’®æ—¶é‡ç½®å†å²è®°å½•ã€è¾“å…¥æ¡†å’Œæ¶ˆæ¯çŠ¶æ€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### ğŸ§© 6. Gradio ç•Œé¢æ„å»º\n",
    "\n",
    "```python\n",
    "msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "```\n",
    "\n",
    "* **äº‹ä»¶ç»‘å®š**ï¼šç”¨æˆ·æäº¤æ–‡æœ¬ â†’ è°ƒç”¨ `respond` â†’ è¿”å›æ–°çŠ¶æ€ã€‚\n",
    "* **Gradio Chatbot ç»„ä»¶**ï¼šä½¿ç”¨ `avatar_images` è®¾ç½®äººæœºå¤´åƒã€‚\n",
    "* **Gradio State**ï¼šè·¨ç»„ä»¶å…±äº«å¹¶æŒä¹…åŒ–æ¶ˆæ¯åˆ—è¡¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "##### âœ… æ€»ç»“\n",
    "\n",
    "| åŠŸèƒ½æ¨¡å—      | å®ç°æ–¹å¼                                              |\n",
    "| --------- | ------------------------------------------------- |\n",
    "| å¯¹è¯æ¨¡å‹      | DeepSeek via `init_chat_model`                    |\n",
    "| Prompt æ¨¡æ¿ | ChatPromptTemplate + System + MessagesPlaceholder |\n",
    "| æ¶ˆæ¯ç®¡ç†      | æ‰‹åŠ¨ç®¡ç† + `gr.State` ä¿å­˜å¹¶è£å‰ªæœ€è¿‘ 50 æ¡                    |\n",
    "| å¤šè½®å¯¹è¯      | ç”¨æˆ·/AI Message åˆ—è¡¨æ„å»ºå¹¶ä¼ å…¥ LCEL é“¾                      |\n",
    "| UI ç•Œé¢     | Gradio Blocks + Chatbot ç»„ä»¶ + æ¸…ç©ºæŒ‰é’®                 |\n",
    "| æµå¼è¾“å‡º      | ä½¿ç”¨ `qa_chain.astream()` æŒç»­ç”Ÿæˆå›å¤                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe3aed-b71c-44c8-98d9-1aaa23ec862b",
   "metadata": {},
   "source": [
    "å½“ç„¶è¿™åªæ˜¯æœ€ç®€å•çš„é—®ç­”æœºå™¨äººå®ç°å½¢å¼ï¼Œå®é™…ä¸Šä¼ä¸šåº”ç”¨çš„é—®ç­”æœºå™¨äººå¾€å¾€éœ€è¦æ›´åŠ å¤æ‚çš„é€»è¾‘ï¼Œæ¯”å¦‚ç”¨æˆ·æƒé™ç®¡ç†ã€ä¸Šä¸‹æ–‡è®°å¿†ç­‰ï¼Œæ›´å¤šå†…å®¹è¯¦è§ã€Šå¤§æ¨¡å‹ä¸Agentå¼€å‘ã€‹è¯¾ç¨‹è®²è§£ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
