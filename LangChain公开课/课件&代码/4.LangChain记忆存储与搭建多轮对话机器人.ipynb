{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd154d49-fbc4-4d18-9259-38e12f6aebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbaad8-3d1b-4fad-9bf5-f7acf73c927c",
   "metadata": {},
   "source": [
    "- **MateGen项目演示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813013ab-33b4-4e16-91a9-146d580eb92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/4.MateGen%20Pro%20%E9%A1%B9%E7%9B%AE%E5%8A%9F%E8%83%BD%E6%BC%94%E7%A4%BA.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/4.MateGen%20Pro%20%E9%A1%B9%E7%9B%AE%E5%8A%9F%E8%83%BD%E6%BC%94%E7%A4%BA.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb508f-888e-4c13-8f06-2494c127ebae",
   "metadata": {},
   "source": [
    "- **智能客服项目演示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b205118-2bfe-4998-ad0f-66f9b2c0928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a871230-0dda-42c2-ad2a-1b0c3539517c",
   "metadata": {},
   "source": [
    "- **Dify项目演示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ababdf0-45e4-4982-8f1c-e964c47fa470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021efd9f-c5bb-4cd8-93b7-0e7242515556",
   "metadata": {},
   "source": [
    "- **LangChain&LangGraph搭建Multi-Agnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f6fdb7-6141-4211-a044-7202fac62a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939d1f3-b2b2-4d38-a588-969f9f7f695a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521dcdc-c8c0-452f-b330-534459b35fa2",
   "metadata": {},
   "source": [
    "# <center> LangChain快速入门与Agent开发实战\n",
    "# <center> Part 4.LangChain记忆存储与搭建多轮对话机器人"
   ]
  },
  {
   "cell_type": "code",
   "id": "904a6dfe-cf33-49a2-8cbb-8d335aad0132",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:40:34.409029Z",
     "start_time": "2025-09-13T13:40:34.399817Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv(override=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6d07d20d-bb4b-4dd8-be89-832a0cf2dd78",
   "metadata": {},
   "source": [
    "### 1. 构建多轮对话的流式智能问答系统"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d628728-e3ee-4e6d-bbec-618aded3d751",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在`langChain`中构建一个基本的问答机器人仅需要使用一个`Chain`便可以快速实现，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "id": "3ed84d9c-20cc-4342-b358-fc204a5fd633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:41:02.182596Z",
     "start_time": "2025-09-13T13:40:53.902192Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你叫小智，是一名乐于助人的助手。\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 使用 DeepSeek 模型\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# 直接使用模型 + 输出解析器\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# 测试\n",
    "question = \"你好，请你介绍一下你自己。\"\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是小智，一个智能助手，随时准备为你提供帮助。我可以回答你的问题、提供信息、协助解决问题，或者陪你聊聊天。无论是日常生活、学习、工作，还是兴趣爱好，我都会尽力为你提供有用的建议和有趣的互动。有什么想问的，尽管告诉我吧！ 😊\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "e14e5896-234a-4240-859e-3d78ac1a1dd8",
   "metadata": {},
   "source": [
    "- 添加多轮对话记忆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44f306-61fe-4d53-952a-75504ac6d385",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在LangChain中，我们可以通过人工拼接消息队列，来为每次模型调用设置多轮对话记忆。"
   ]
  },
  {
   "cell_type": "code",
   "id": "79fdd8dc-a164-47b3-be21-76895d6f596f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:42:23.796991Z",
     "start_time": "2025-09-13T13:42:23.793848Z"
    }
   },
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9790b487-82ac-4c10-b777-fd1bbfa6caa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:45:46.315635Z",
     "start_time": "2025-09-13T13:45:46.312621Z"
    }
   },
   "source": [
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"你叫小智，是一名乐于助人的助手。\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "cc89120b-1ee2-405c-82c5-6e276784684d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:46:14.402173Z",
     "start_time": "2025-09-13T13:46:14.399851Z"
    }
   },
   "source": [
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "15120163-e597-4145-b3fd-38aeeff4ce90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:47:04.838781Z",
     "start_time": "2025-09-13T13:47:04.835855Z"
    }
   },
   "source": [
    "messages_list = [\n",
    "    HumanMessage(content=\"你好，我叫陈明，好久不见。\"),\n",
    "    AIMessage(content=\"你好呀！我是小智，一名乐于助人的AI助手。很高兴认识你！\"),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "50271ead-0857-49aa-83d8-aefa66194fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:47:10.737684Z",
     "start_time": "2025-09-13T13:47:10.735602Z"
    }
   },
   "source": [
    "question = \"你好，请问我叫什么名字。\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "40fbabe7-44dd-4635-89ab-a2ce38019b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:47:21.601784Z",
     "start_time": "2025-09-13T13:47:21.598653Z"
    }
   },
   "source": [
    "messages_list.append(HumanMessage(content=question))"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "4c6699fe-7292-4c65-9280-ae84cbb030b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:47:24.219082Z",
     "start_time": "2025-09-13T13:47:24.214872Z"
    }
   },
   "source": [
    "messages_list"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='你好，我叫陈明，好久不见。', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='你好呀！我是小智，一名乐于助人的AI助手。很高兴认识你！', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='你好，请问我叫什么名字。', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "44a2a038-9520-422c-86dc-36acb108bc03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:48:04.154487Z",
     "start_time": "2025-09-13T13:48:00.010436Z"
    }
   },
   "source": [
    "result = basic_qa_chain.invoke({\"messages\": messages_list})\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你刚刚告诉我你叫陈明呀！很高兴认识你，陈明！有什么我可以帮你的吗？ 😊\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "76b11106-bdaf-4c96-a407-a85ca2a44933",
   "metadata": {},
   "source": [
    "完整的多轮对话函如下："
   ]
  },
  {
   "cell_type": "code",
   "id": "6cc997f1-8a6f-47a6-abbb-edda2a410970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:51:03.344951Z",
     "start_time": "2025-09-13T13:48:57.954586Z"
    }
   },
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model  = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"你叫小智，是一名乐于助人的助手。\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "messages_list = []  # 初始化历史\n",
    "print(\"🔹 输入 exit 结束对话\")\n",
    "while True:\n",
    "    user_query = input(\"👤 你：\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    # 1) 追加用户消息\n",
    "    messages_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    # 2) 调用模型\n",
    "    assistant_reply = chain.invoke({\"messages\": messages_list})\n",
    "    print(\"🤖 小智：\", assistant_reply)\n",
    "\n",
    "    # 3) 追加 AI 回复\n",
    "    messages_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    # 4) 仅保留最近 50 条\n",
    "    messages_list = messages_list[-50:]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 输入 exit 结束对话\n",
      "🤖 小智： 你好，邹一苇！很高兴认识你。请问有什么可以帮你的吗？\n",
      "🤖 小智： 25岁是充满可能性的年纪呢！刚开始工作可能会有些挑战，但也是快速学习和成长的黄金时期。如果需要交流职场适应、时间管理或是任何方面的经验，我很乐意为你提供建议～ 😊 目前从事什么行业呢？\n",
      "🤖 小智： 很棒的选择！金融科技领域现在发展非常迅速，基金行业尤其需要技术人才来支持数据分析、交易系统和风控平台的开发。作为刚入行的开发者，或许可以多关注：\n",
      "\n",
      "1. **金融业务知识** - 了解基金净值计算、投资组合管理等基础概念会对开发更有帮助\n",
      "2. **技术栈特点** - 金融系统通常对并发处理、数据一致性有较高要求\n",
      "3. **合规安全** - 金融行业对数据安全和监管合规非常重视\n",
      "\n",
      "最近在接触什么类型的项目呢？遇到具体技术问题时也欢迎随时交流~ 💻\n",
      "🤖 小智： 当然记得！您刚才提到过：  \n",
      "**姓名**：邹一苇  \n",
      "**年龄**：25岁  \n",
      "**职业**：软件开发工程师（目前就职于基金行业）  \n",
      "\n",
      "如果有需要补充或修正的地方，请随时告诉我～ 😊\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "ea5f7255-73e8-4933-a835-e0b4f71bb295",
   "metadata": {},
   "source": [
    "- 流式打印聊天信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a598492-1007-4302-a00f-22397a42a22f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外还有一个问题是，大家经常看到的问答机器人其实都是采用流式传输模式。用户输入问题，等待模型直接返回回答，然后用户再输入问题，模型再返回回答，这样循环下去，用户输入问题和模型返回回答之间的时间间隔太长，导致用户感觉机器人反应很慢。所以`LangChain`提供了一个`astream`方法，可以实现流式输出，即一旦模型有输出，就立即返回，这样用户就可以看到模型正在思考，而不是等待模型思考完再返回。\n",
    "\n",
    "\n",
    "&emsp;&emsp;实现的方法也非常简单，只需要在调用模型时将`invoke`方法替换为`astream`方法，然后使用`async for`循环来获取模型的输出即可。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "id": "2e23ae26-75b4-467c-a522-58586cd8b58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:52:33.755645Z",
     "start_time": "2025-09-13T13:52:26.003385Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你叫小智，是一名乐于助人的助手。\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 使用 DeepSeek 模型\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# 直接使用提示模版 +模型 + 输出解析器\n",
    "qa_chain_with_system = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# 异步实现流式输出\n",
    "async for chunk in qa_chain_with_system.astream({\"input\": \"你好，请你介绍一下你自己\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是小智，一名智能助手，随时准备为你提供帮助。我可以回答你的问题、提供信息、协助解决问题，或者陪你聊天。无论是学习、工作还是生活中的疑问，我都会尽力给出清晰、准确的回答。我的知识覆盖广泛，包括科技、文化、教育、娱乐等多个领域，并且会不断更新。如果有任何需要，请随时告诉我！ 😊"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6ab2caa-3fc1-4293-91dd-d68dad16f5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 输入 exit 结束对话\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👤 你： 你好，我叫陈明，好久不见\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好啊陈明！确实好久不见了，最近过得怎么样？工作还顺利吗？记得上次聊天时你好像正在准备一个重要的项目，现在应该已经顺利完成了吧？有什么新鲜事想和我分享的吗？"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👤 你： 请问，你还记得我叫什么名字么？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（突然进入「名侦探模式」）  \n",
      "\n",
      "陈明同学，这可是道送分题！✨ 虽然我的记忆像金鱼一样只有7秒，但当前对话中你刚刚强调过——  \n",
      "\n",
      "**「陈明」** 这两个字已经用荧光笔标在我脑海的小黑板上了！(๑•̀ㅂ•́)و✧  \n",
      "\n",
      "（不过如果现在你突然说「其实我叫张大勇」…我也会立刻乖巧改口的hhh）"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "👤 你： exit\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"你叫小智，是一名乐于助人的助手。\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "messages_list = []  # 初始化历史\n",
    "print(\"🔹 输入 exit 结束对话\")\n",
    "while True:\n",
    "    user_query = input(\"👤 你：\")\n",
    "    if user_query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    # 1) 追加用户消息\n",
    "    messages_list.append(HumanMessage(content=user_query))\n",
    "\n",
    "    # 2) 调用模型\n",
    "    async for chunk in chain.astream({\"messages\": messages_list}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    # 3) 追加 AI 回复\n",
    "    messages_list.append(AIMessage(content=assistant_reply))\n",
    "\n",
    "    # 4) 仅保留最近 50 条\n",
    "    messages_list = messages_list[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aba029-d360-4ab9-860c-590508217148",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如上所示展示的问答效果就是我们在构建大模型应用时需要实现的流式输出效果。接下来我们就进一步地，使用`gradio`来开发一个支持在网页上进行交互的问答机器人。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0b5d9-d30d-475b-aae5-8e90766eae3b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先需要安装一下`gradio`的第三方依赖包，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48b5f6d6-f9c1-43a9-9197-108ea4f4dcd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.33.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.10.2 (from gradio)\n",
      "  Using cached gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markupsafe<4.0,>=2.0 (from gradio)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting numpy<3.0,>=1.0 (from gradio)\n",
      "  Downloading numpy-2.3.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: orjson~=3.0 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (24.2)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Downloading pandas-2.3.0-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow<12.0,>=8.0 (from gradio)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (2.11.5)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.13-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from gradio) (4.14.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from gradio-client==1.10.2->gradio)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.10.2->gradio)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: certifi in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Collecting filelock (from huggingface-hub>=0.28.1->gradio)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\01_木羽研发\\11_trafficvideo\\langchain_venv\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Downloading gradio-5.33.0-py3-none-any.whl (54.2 MB)\n",
      "   ---------------------------------------- 0.0/54.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/54.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/54.2 MB 2.4 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 1.6/54.2 MB 4.0 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 4.5/54.2 MB 7.9 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 8.4/54.2 MB 11.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 13.1/54.2 MB 13.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 17.3/54.2 MB 14.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 21.5/54.2 MB 15.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 26.0/54.2 MB 16.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 30.4/54.2 MB 16.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 34.9/54.2 MB 17.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 39.1/54.2 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 43.5/54.2 MB 18.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 48.2/54.2 MB 18.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 52.4/54.2 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 54.2/54.2 MB 18.1 MB/s eta 0:00:00\n",
      "Using cached gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading numpy-2.3.0-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 6.0/12.7 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.0-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.0/11.0 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.0 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 21.4 MB/s eta 0:00:00\n",
      "Using cached pillow-11.2.1-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading ruff-0.11.13-py3-none-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 6.0/11.5 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.5 MB 25.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 22.6 MB/s eta 0:00:00\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "Using cached ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pytz, pydub, websockets, tzdata, tomlkit, shellingham, semantic-version, ruff, python-multipart, pillow, numpy, mdurl, markupsafe, groovy, fsspec, filelock, ffmpy, click, aiofiles, uvicorn, starlette, pandas, markdown-it-py, jinja2, huggingface-hub, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "\n",
      "   ----------------------------------------  0/31 [pytz]\n",
      "   --- ------------------------------------  3/31 [tzdata]\n",
      "   --- ------------------------------------  3/31 [tzdata]\n",
      "   --------- ------------------------------  7/31 [ruff]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ----------- ----------------------------  9/31 [pillow]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ------------ --------------------------- 10/31 [numpy]\n",
      "   ---------------- ----------------------- 13/31 [groovy]\n",
      "   ------------------ --------------------- 14/31 [fsspec]\n",
      "   ------------------------ --------------- 19/31 [uvicorn]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   ---------------------------- ----------- 22/31 [markdown-it-py]\n",
      "   ------------------------------ --------- 24/31 [huggingface-hub]\n",
      "   ------------------------------ --------- 24/31 [huggingface-hub]\n",
      "   -------------------------------- ------- 25/31 [safehttpx]\n",
      "   --------------------------------- ------ 26/31 [rich]\n",
      "   ------------------------------------ --- 28/31 [fastapi]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   ---------------------------------------- 31/31 [gradio]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 click-8.2.1 fastapi-0.115.12 ffmpy-0.6.0 filelock-3.18.0 fsspec-2025.5.1 gradio-5.33.0 gradio-client-1.10.2 groovy-0.1.2 huggingface-hub-0.32.4 jinja2-3.1.6 markdown-it-py-3.0.0 markupsafe-3.0.2 mdurl-0.1.2 numpy-2.3.0 pandas-2.3.0 pillow-11.2.1 pydub-0.25.1 python-multipart-0.0.20 pytz-2025.2 rich-14.0.0 ruff-0.11.13 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.46.2 tomlkit-0.13.3 typer-0.16.0 tzdata-2025.2 uvicorn-0.34.3 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "# 安装 Gradio\n",
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6c086-e0c5-4a25-a477-97ec2525d04f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;完整实现的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "id": "5216a90c-739b-42d6-8ad4-d55cf8ca0001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T13:59:08.373885Z",
     "start_time": "2025-09-13T13:58:56.809779Z"
    }
   },
   "source": [
    "import gradio as gr\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 1. 模型、Prompt、Chain\n",
    "# ──────────────────────────────────────────────\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"你叫小智，是一名乐于助人的助手。\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # 手动传入历史\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = chatbot_prompt | model | parser   # LCEL 组合\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 2. Gradio 组件\n",
    "# ──────────────────────────────────────────────\n",
    "CSS = \"\"\"\n",
    ".main-container {max-width: 1200px; margin: 0 auto; padding: 20px;}\n",
    ".header-text {text-align: center; margin-bottom: 20px;}\n",
    "\"\"\"\n",
    "\n",
    "def create_chatbot() -> gr.Blocks:\n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=CSS) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            gr.Markdown(\"# 🤖 LangChain B站公开课 By九天Hector\", elem_classes=[\"header-text\"])\n",
    "            gr.Markdown(\"基于 LangChain LCEL 构建的流式对话机器人\", elem_classes=[\"header-text\"])\n",
    "\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\",\n",
    "                ),\n",
    "            )\n",
    "            msg = gr.Textbox(placeholder=\"请输入您的问题...\", container=False, scale=7)\n",
    "            submit = gr.Button(\"发送\", scale=1, variant=\"primary\")\n",
    "            clear = gr.Button(\"清空\", scale=1)\n",
    "\n",
    "        # ---------------  状态：保存 messages_list  ---------------\n",
    "        state = gr.State([])          # 这里存放真正的 Message 对象列表\n",
    "\n",
    "        # ---------------  主响应函数（流式） ----------------------\n",
    "        async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "            # 1) 输入为空直接返回\n",
    "            if not user_msg.strip():\n",
    "                yield \"\", chat_hist, messages_list\n",
    "                return\n",
    "\n",
    "            # 2) 追加用户消息\n",
    "            messages_list.append(HumanMessage(content=user_msg))\n",
    "            chat_hist = chat_hist + [(user_msg, None)]\n",
    "            yield \"\", chat_hist, messages_list      # 先显示用户消息\n",
    "\n",
    "            # 3) 流式调用模型\n",
    "            partial = \"\"\n",
    "            async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "                partial += chunk\n",
    "                # 更新最后一条 AI 回复\n",
    "                chat_hist[-1] = (user_msg, partial)\n",
    "                yield \"\", chat_hist, messages_list\n",
    "\n",
    "            # 4) 完整回复加入历史，裁剪到最近 50 条\n",
    "            messages_list.append(AIMessage(content=partial))\n",
    "            messages_list = messages_list[-50:]\n",
    "\n",
    "            # 5) 最终返回（Gradio 需要把新的 state 传回）\n",
    "            yield \"\", chat_hist, messages_list\n",
    "\n",
    "        # ---------------  清空函数 -------------------------------\n",
    "        def clear_history():\n",
    "            return [], \"\", []          # 清空 Chatbot、输入框、messages_list\n",
    "\n",
    "        # ---------------  事件绑定 ------------------------------\n",
    "        msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 3. 启动应用\n",
    "# ──────────────────────────────────────────────\n",
    "demo = create_chatbot()\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=None, share=False, debug=True)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_26808\\1965123637.py:36: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7860): [winerror 10048] 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7861): [winerror 10048] 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Couldn't start the app because 'http://localhost:7862/gradio_api/startup-events' failed (code 502). Check your network or proxy settings to ensure localhost is accessible.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mException\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 94\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;66;03m# ──────────────────────────────────────────────\u001B[39;00m\n\u001B[32m     91\u001B[39m \u001B[38;5;66;03m# 3. 启动应用\u001B[39;00m\n\u001B[32m     92\u001B[39m \u001B[38;5;66;03m# ──────────────────────────────────────────────\u001B[39;00m\n\u001B[32m     93\u001B[39m demo = create_chatbot()\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[43mdemo\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlaunch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mserver_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m0.0.0.0\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserver_port\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshare\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Anaconda\\envs\\langchain\\Lib\\site-packages\\gradio\\blocks.py:2707\u001B[39m, in \u001B[36mBlocks.launch\u001B[39m\u001B[34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001B[39m\n\u001B[32m   2701\u001B[39m     resp = httpx.get(\n\u001B[32m   2702\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.local_api_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mstartup-events\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2703\u001B[39m         verify=ssl_verify,\n\u001B[32m   2704\u001B[39m         timeout=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   2705\u001B[39m     )\n\u001B[32m   2706\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m resp.is_success:\n\u001B[32m-> \u001B[39m\u001B[32m2707\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[32m   2708\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCouldn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt start the app because \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresp.url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m failed (code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresp.status_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m). Check your network or proxy settings to ensure localhost is accessible.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2709\u001B[39m         )\n\u001B[32m   2710\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2711\u001B[39m     \u001B[38;5;66;03m# NOTE: One benefit of the code above dispatching `startup_events()` via a self HTTP request is\u001B[39;00m\n\u001B[32m   2712\u001B[39m     \u001B[38;5;66;03m# that `self._queue.start()` is called in another thread which is managed by the HTTP server, `uvicorn`\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2715\u001B[39m     \u001B[38;5;66;03m# In contrast, in the Wasm env, we can't do that because `threading` is not supported and all async tasks will run in the same event loop, `pyodide.webloop.WebLoop` in the main thread.\u001B[39;00m\n\u001B[32m   2716\u001B[39m     \u001B[38;5;66;03m# So we need to manually cancel them. See `self.close()`..\u001B[39;00m\n\u001B[32m   2717\u001B[39m     \u001B[38;5;28mself\u001B[39m.run_startup_events()\n",
      "\u001B[31mException\u001B[39m: Couldn't start the app because 'http://localhost:7862/gradio_api/startup-events' failed (code 502). Check your network or proxy settings to ensure localhost is accessible."
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "fb3c47bc-59d5-4339-8a4c-4231b37e5eeb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;运行后，在浏览器访问`http://127.0.0.1:7860`即可进行问答交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd576d-4cbe-4b0b-b4e6-233a70f06719",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202506121740968.png\" alt=\"image-20250612174010864\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23804aae-73f4-4e81-b310-7675ec653d8d",
   "metadata": {},
   "source": [
    "具体代码解释如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52a976-a597-49f1-80a1-23baa1de38b6",
   "metadata": {},
   "source": [
    "##### 🧱 1. 模块说明\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import gradio as gr\n",
    "```\n",
    "\n",
    "* `init_chat_model`：初始化 DeepSeek 等聊天模型。\n",
    "* `ChatPromptTemplate`：用于构建聊天 Prompt 模板。\n",
    "* `MessagesPlaceholder`：用于占位历史消息。\n",
    "* `HumanMessage` / `AIMessage`：构建多轮消息结构。\n",
    "* `StrOutputParser`：将模型输出转换为字符串。\n",
    "* `gradio`：构建网页界面。\n",
    "\n",
    "---\n",
    "\n",
    "##### 🧠 2. Prompt 构建与模型初始化\n",
    "\n",
    "```python\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"你叫小智，是一名乐于助人的助手。\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "qa_chain = chatbot_prompt | model | parser\n",
    "```\n",
    "\n",
    "* **SystemMessage**：初始化系统角色设定（小智）。\n",
    "* **MessagesPlaceholder**：用变量名 `messages` 占位历史消息。\n",
    "* **qa\\_chain**：组合为 LangChain Expression Language 链。\n",
    "\n",
    "---\n",
    "\n",
    "##### 🔄 3. 手动管理消息列表\n",
    "\n",
    "```python\n",
    "state = gr.State([])\n",
    "```\n",
    "\n",
    "我们用 `gr.State` 存储所有历史消息（列表）。每次用户发送消息，都会：\n",
    "\n",
    "* append 一个 `HumanMessage`。\n",
    "* 流式调用模型并不断更新回复。\n",
    "* append 一个 `AIMessage`。\n",
    "* 最后裁剪：`messages_list = messages_list[-50:]`。\n",
    "\n",
    "---\n",
    "\n",
    "##### 🌊 4. 流式响应函数\n",
    "\n",
    "```python\n",
    "async def respond(user_msg: str, chat_hist: list, messages_list: list):\n",
    "    if not user_msg.strip():\n",
    "        yield \"\", chat_hist, messages_list\n",
    "        return\n",
    "\n",
    "    messages_list.append(HumanMessage(content=user_msg))\n",
    "    chat_hist = chat_hist + [(user_msg, None)]\n",
    "    yield \"\", chat_hist, messages_list\n",
    "\n",
    "    partial = \"\"\n",
    "    async for chunk in qa_chain.astream({\"messages\": messages_list}):\n",
    "        partial += chunk\n",
    "        chat_hist[-1] = (user_msg, partial)\n",
    "        yield \"\", chat_hist, messages_list\n",
    "\n",
    "    messages_list.append(AIMessage(content=partial))\n",
    "    messages_list = messages_list[-50:]\n",
    "    yield \"\", chat_hist, messages_list\n",
    "```\n",
    "\n",
    "* **支持 async 流式输出**。\n",
    "* **动态更新最后一轮对话**。\n",
    "* **通过 `yield` 实时反馈到前端**。\n",
    "\n",
    "---\n",
    "\n",
    "##### 🧼 5. 清空历史函数\n",
    "\n",
    "```python\n",
    "def clear_history():\n",
    "    return [], \"\", []\n",
    "```\n",
    "\n",
    "用于点击 \"清空\" 按钮时重置历史记录、输入框和消息状态。\n",
    "\n",
    "---\n",
    "\n",
    "##### 🧩 6. Gradio 界面构建\n",
    "\n",
    "```python\n",
    "msg.submit(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "submit.click(respond, [msg, chatbot, state], [msg, chatbot, state])\n",
    "clear.click(clear_history, outputs=[chatbot, msg, state])\n",
    "```\n",
    "\n",
    "* **事件绑定**：用户提交文本 → 调用 `respond` → 返回新状态。\n",
    "* **Gradio Chatbot 组件**：使用 `avatar_images` 设置人机头像。\n",
    "* **Gradio State**：跨组件共享并持久化消息列表。\n",
    "\n",
    "---\n",
    "\n",
    "##### ✅ 总结\n",
    "\n",
    "| 功能模块      | 实现方式                                              |\n",
    "| --------- | ------------------------------------------------- |\n",
    "| 对话模型      | DeepSeek via `init_chat_model`                    |\n",
    "| Prompt 模板 | ChatPromptTemplate + System + MessagesPlaceholder |\n",
    "| 消息管理      | 手动管理 + `gr.State` 保存并裁剪最近 50 条                    |\n",
    "| 多轮对话      | 用户/AI Message 列表构建并传入 LCEL 链                      |\n",
    "| UI 界面     | Gradio Blocks + Chatbot 组件 + 清空按钮                 |\n",
    "| 流式输出      | 使用 `qa_chain.astream()` 持续生成回复                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe3aed-b71c-44c8-98d9-1aaa23ec862b",
   "metadata": {},
   "source": [
    "当然这只是最简单的问答机器人实现形式，实际上企业应用的问答机器人往往需要更加复杂的逻辑，比如用户权限管理、上下文记忆等，更多内容详见《大模型与Agent开发》课程讲解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
